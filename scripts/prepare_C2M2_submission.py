#!/usr/bin/env python3

##########################################################################################
#                                          SCRIPT PROVENANCE
##########################################################################################
# 
# Arthur Brady (University of Maryland Institute for Genome Sciences) wrote this script to
# scan and summarize (via term-tracking TSV files) controlled-vocabulary term usage across
# a C2M2 instance to assist with datapackage preparation prior to submission for
# publication in the CFDE portal.
# 
# Creation date: 2020-05-17
# Lastmod date unless I forgot to change it: 2022-09-23
# 
##########################################################################################

import os
import json
import re
import sys
import gzip
import subprocess
from pathlib import Path

##########################################################################################
##########################################################################################
##########################################################################################
#                                          USER-DEFINED PARAMETERS
##########################################################################################
##########################################################################################
##########################################################################################

##########################################################################################
# Directory containing full CV reference info (see below, 'cvFile' dictionary, for file
# list).

cvRefDir = 'external_CV_reference_files'

##########################################################################################
# Directory in which C2M2 core submission TSVs (for the purposes of this script,
# this means 'file.tsv' and 'biosample.tsv') have been built and stored, prior to running
# this script.

submissionDraftDir = 'draft_C2M2_submission_TSVs'

##########################################################################################
# Directory into which new TSVs generated by this script will be written summarizing all
# controlled vocabulary term usage throughout this C2M2 instance.

outDir = 'autogenerated_C2M2_term_tables'

##########################################################################################
##########################################################################################
##########################################################################################
#                                  CONSTANT PARAMETERS: DO NOT MODIFY
##########################################################################################
##########################################################################################
##########################################################################################

##########################################################################################
# Map of CV names to reference files. These files should be present in cvRefDir before
# running this script.

cvFile = {
    
    'EDAM' : '%s/EDAM.version_1.25.tsv' % cvRefDir,
    'Interlex_file_format' : '%s/Interlex_file_format.2022-11-01.tsv' % cvRefDir,
    'Interlex_data_type' : '%s/Interlex_data_type.2022-11-01.tsv' % cvRefDir,
    'NCBI' : '%s/ncbi_taxonomy.tsv.gz' % cvRefDir,
    'OBI' : '%s/OBI.version_2022-07-11.obo' % cvRefDir,
    'OBI_provisional' : '%s/OBI.provisional_terms.2022-09-23.tsv' % cvRefDir,
    'Uberon' : '%s/uberon.version_2021-11-12.obo' % cvRefDir,
    'DO' : '%s/doid.version_2021-10-12.obo' % cvRefDir,
    'HPO' : '%s/hp.2022-02-14.obo' % cvRefDir,
    'Ensembl' : '%s/ensembl_genes.tsv' % cvRefDir,
    # Reduced-size (sample) PubChem reference DB for debugging and fast testing (if you enable this, you must also comment out the line after it):
    #'PubChem_and_GlyTouCan_compound' : '%s/sample_pubchem_reference_data/compound.first_5000_records.max_100_synonyms_per_term.tsv.gz' % cvRefDir,
    'PubChem_and_GlyTouCan_compound' : '%s/compound.tsv.gz' % cvRefDir,
    # Reduced-size (sample) PubChem reference DB for debugging and fast testing (if you enable this, you must also comment out the line after it):
    #'PubChem_substance' : '%s/sample_pubchem_reference_data/substance.records_for_first_5000_CIDs.max_100_synonyms_per_term.tsv.gz' % cvRefDir,
    'PubChem_substance' : '%s/substance.tsv.gz' % cvRefDir,
    # Reduced-size (sample) UniProtKB reference DB for debugging and fast testing (if you enable this, you must also comment out the line after it):
    #'UniProtKB' : '%s/sample_uniprot_reference_data/protein.first_5000_records.tsv.gz' % cvRefDir
    'UniProtKB' : '%s/protein.tsv.gz' % cvRefDir
}

##########################################################################################
# TSV filenames to scan for term usage. These files should be present in
# submissionDraftDir before running this script.

targetTSVs = (
    
    'file.tsv',
    
    'biosample.tsv',
    'biosample_disease.tsv',
    'biosample_gene.tsv',
    'biosample_substance.tsv',
    
    'subject_disease.tsv',
    'subject_phenotype.tsv',
    'subject_role_taxonomy.tsv',
    'subject_substance.tsv',
    
    'collection_anatomy.tsv',
    'collection_compound.tsv',
    'collection_disease.tsv',
    'collection_gene.tsv',
    'collection_phenotype.tsv',
    'collection_protein.tsv',
    'collection_substance.tsv',
    'collection_taxonomy.tsv'
)

##########################################################################################
# Term-tracker data structure.

termsUsed = {
    
    'anatomy': {},
    'assay_type': {},
    'analysis_type': {},
    'compound': {},
    'data_type': {},
    'disease': {},
    'file_format': {},
    'gene': {},
    'ncbi_taxonomy': {},
    'phenotype': {},
    'protein': {},
    'sample_prep_method': {},
    'substance': {}
}

##########################################################################################
# ID map files.

idMap = {
    
    'entrez_to_ensembl': '%s/HPO_Entrez_gene_IDs_to_EnsEMBL_IDs.tsv' % cvRefDir,
    'hpo_to_entrez': '%s/hp.phenotype_to_genes.txt' % cvRefDir
}

##########################################################################################
# Auto-constructed C2M2 reference maps from HPO.

refMap = {
    
    'phenotype': {
        
        'disease': {},
        'gene': {}
    },
    'protein': {
        
        'gene': {}
    }
}

##########################################################################################
# Cache of error reports for deferredDie().

errorStrings = []

##########################################################################################
##########################################################################################
##########################################################################################
#                         SUBROUTINES (in call order, including recursion)
##########################################################################################
##########################################################################################
##########################################################################################

####### progressReport ###################################################################
# 
# CALLED BY: main execution thread
# 
# Print a logging message to STDERR.
# 
#-----------------------------------------------------------------------------------------

def progressReport( message ):
     
     print('%s' % message, file=sys.stdout)

#-----------------------------------------------------------------------------------------
# end sub: progressReport( message )
##########################################################################################

####### die ##############################################################################
# 
# CALLED BY: [everything....]
# Halt program and report why.
# 
#-----------------------------------------------------------------------------------------

def die( errorMessage ):
    
    print('\n   FATAL: %s' % errorMessage, file=sys.stderr)

    sys.exit(-1)

#-----------------------------------------------------------------------------------------
# end sub: die( errorMessage )
##########################################################################################

####### cacheError #######################################################################
# 
# CALLED BY: [everything....]
# Save a detected error for downstream aggregate reporting.
# 
#-----------------------------------------------------------------------------------------

def cacheError( errorString ):
    
    global errorStrings

    errorStrings.append(errorString)

#-----------------------------------------------------------------------------------------
# end sub: cacheError( )
##########################################################################################


####### deferredDie ######################################################################
# 
# CALLED BY: [everything....]
# Halt program and report why after waiting til a block of similar stuff has finished.
# 
#-----------------------------------------------------------------------------------------

def deferredDie(  ):
    
    global errorStrings

    if len(errorStrings) > 1:
        
        print('\n   FATAL (multiple errors):\n', file=sys.stderr)

    else:
        
        print('\n   FATAL:\n', file=sys.stderr)

    for errorString in errorStrings:
        print('      %s' % errorString, file=sys.stderr)

    print('', file=sys.stderr)

    sys.exit(-1)

#-----------------------------------------------------------------------------------------
# end sub: deferredDie( )
##########################################################################################

def identifyTermsUsed(  ):
    
    global termsUsed, submissionDraftDir, targetTSVs

    progressReport("Loading all CV terms used in submission tables... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]\n")

    for basename in targetTSVs:
        
        inFile = submissionDraftDir + '/' + basename

        if Path(inFile).is_file():
            
            progressReport("   scanning \"" + inFile + "\"...")

            with open( inFile, 'r' ) as IN:
                
                header = IN.readline()

                colNames = re.split(r'\t', header.rstrip('\r\n'))

                currentColIndex = 0

                columnToCategory = dict()

                for colName in colNames:
                    
                    if basename == 'biosample.tsv' and colName == 'assay_type':
                        
                        die('The biosample.assay_type field is now deprecated; please use biosample.sample_prep_method (or file.assay_type) instead. Note: any past submissions containing biosample.assay_type that were previously published to the CFDE portal DO NOT have to be refactored just to accommodate this change: you only need to update biosample.tsv if you\'re preparing a new or updated submission. Unaltered past submissions will continue to be browsable in the portal (minus any data in the deprecated biosample.assay_type field). See the wiki for up-to-date usage details for biosample.tsv.')

                    elif colName in termsUsed:
                        
                        columnToCategory[currentColIndex] = colName

                    elif basename == 'subject_role_taxonomy.tsv' and colName == 'taxonomy_id':
                        
                        columnToCategory[currentColIndex] = 'ncbi_taxonomy'

                    elif basename == 'collection_taxonomy.tsv' and colName == 'taxon':
                        
                        columnToCategory[currentColIndex] = 'ncbi_taxonomy'

                    elif basename == 'file.tsv' and colName == 'compression_format':
                        
                        columnToCategory[currentColIndex] = 'file_format'

                    currentColIndex += 1

                for line in IN:
                    
                    fields = re.split(r'\t', line.rstrip('\r\n'))

                    for colIndex in columnToCategory:
                        
                        currentCategory = columnToCategory[colIndex]

                        if fields[colIndex] != '':
                            
                            termsUsed[currentCategory][fields[colIndex]] = {}

                # end for ( line iterator on input TSV )

            # end with ( open input TSV as IN )

        # end if ( input TSV exists )

    # end for ( basename in target TSV list )

    progressReport("\n...done scanning all CV terms used in this submission. [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

# end sub: identifyTermsUsed(  )

def decorateTermsUsed(  ):
    
    global termsUsed, cvFile, idMap, refMap

    categories = list(termsUsed.keys())

    progressReport("Loading display data (names, descriptions, etc.) for all CV terms used in submission tables... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]\n")

    # Phenotype next: these terms determine sets of extra gene and disease terms autoloaded
    # into phenotype_gene and phenotype_disease; we collect and pre-load all usable terms
    # directly from the HPO data.

    progressReport("   Human Phenotype Ontology... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

    # Establish which Ensembl genes we actually know about.

    existing_ensembl_ids = {}

    cv = 'Ensembl'

    refFile = cvFile[cv]
        
    with open( refFile, 'r' ) as IN:
        
        header = IN.readline()

        for line in IN:
            
            line = line.rstrip('\r\n')

            ( currentTerm, currentName, currentDesc, currentSyn, currentTaxID ) = re.split(r'\t', line)

            existing_ensembl_ids[currentTerm] = 1

        # end ( line iterator on Ensembl gene reference TSV)

    # end ( with open( refFile, 'r' ) as IN )

    # Load a map from HPO-supplied Entrez Gene IDs to EnsEMBL IDs.

    mapFile = idMap['entrez_to_ensembl']

    entrez_to_ensembl = {}

    with open( mapFile, 'r' ) as IN:
        
        header = IN.readline()

        for line in IN:
            
            line = line.rstrip('\r\n')

            ( entrez_id, ensembl_ids ) = re.split(r'\t', line)

            ensembl_id_array = re.split(r'\|', ensembl_ids)

            for ensembl_id in ensembl_id_array:
                
                if ( not ( re.search(r'^ENSG', ensembl_id) is None ) ) and ( ensembl_id in existing_ensembl_ids ):
                    
                    if not ( entrez_id in entrez_to_ensembl ):
                        
                        entrez_to_ensembl[entrez_id] = {}

                    entrez_to_ensembl[entrez_id][ensembl_id] = 1

            # end prefix check on each current ensembl ID target

        # end ( line iterator on the entrez-to-ensembl map )

    # end with ( open mapFile as IN )

    # Load a map from HPO terms to Entrez Gene IDs (map supplied by HPO).

    mapFile = idMap['hpo_to_entrez']

    hpo_to_entrez = {}

    with open( mapFile, 'r' ) as IN:
        
        header = IN.readline()

        for line in IN:
            
            line = line.rstrip('\r\n')

            fields = re.split(r'\t', line)

            hpo_id = fields[0]

            entrez_id = fields[2]

            if not ( hpo_id in hpo_to_entrez ):
                
                hpo_to_entrez[hpo_id] = {}

            hpo_to_entrez[hpo_id][entrez_id] = 1

        # end ( line iterator on the hpo-to-entrez map )

    # end with ( open mapFile as IN )

    cv = 'HPO'

    refFile = cvFile[cv]

    categoryID = 'phenotype'

    with open( refFile, 'r' ) as IN:
        
        recording = False

        currentTerm = ''

        for line in IN:
            
            line = line.rstrip('\r\n')
        
            matchResult = re.search(r'^id:\s+(\S.*)$', line)

            if not( matchResult is None ):
                
                currentTerm = matchResult.group(1)

                if currentTerm in termsUsed[categoryID]:
                    
                    recording = True

                    # Store phenotype-gene links for this term.

                    if ( currentTerm in hpo_to_entrez ) and not ( currentTerm in refMap['phenotype']['gene'] ):
                        
                        for entrez_id in hpo_to_entrez[currentTerm]:
                            
                            if entrez_id in entrez_to_ensembl:
                                
                                # We only get here if the target IDs actually exist in our Ensembl reference TSV.

                                refMap['phenotype']['gene'][currentTerm] = {}

                                for ensembl_id in entrez_to_ensembl[entrez_id]:
                                    
                                    refMap['phenotype']['gene'][currentTerm][ensembl_id] = 1
                                    termsUsed['gene'][ensembl_id] = {}

                else:
                    
                    currentTerm = ''

                    # (Recording is already switched off by default.)

            elif not( re.search(r'^\[Term\]', line) is None ):
                
                recording = False

            elif not( re.search(r'^\[Typedef\]', line) is None ):
                
                break

            elif recording:
                
                if not ( re.search(r'^name:\s+(\S*.*)$', line) is None ):
                    
                    termsUsed[categoryID][currentTerm]['name'] = re.search(r'^name:\s+(\S*.*)$', line).group(1)

                elif not ( re.search(r'^def:\s+\"(.*)\"[^\"]*$', line) is None ):
                    
                    parsedDesc = re.search(r'^def:\s+\"(.*?)(?<!\\)\".*$', line).group(1)

                    parsedDesc = re.sub(r'\s*\[[^\]]+\]', r'', parsedDesc)

                    # Remove newline codes and replace with space characters.

                    parsedDesc = re.sub(r'\\n', r' ', parsedDesc)

                    # Trim remaining extremal whitespace.

                    parsedDesc = re.sub(r'^\s+', r'', parsedDesc)
                    parsedDesc = re.sub(r'\s+$', r'', parsedDesc)

                    termsUsed[categoryID][currentTerm]['description'] = parsedDesc

                elif not ( re.search(r'^alt_id:\s+\S+', line) is None ):
                    
                    newSyn = re.search(r'^alt_id:\s+(\S+)', line).group(1)

                    newSyn = re.sub(r'\"', r'', newSyn)
                    newSyn = re.sub(r'\'', r'', newSyn)

                    if re.search(r'\|', newSyn) is None:
                        
                        if 'synonyms' in termsUsed[categoryID][currentTerm]:
                            
                            termsUsed[categoryID][currentTerm]['synonyms'] = termsUsed[categoryID][currentTerm]['synonyms'] + '|"' + newSyn + '"'

                        else:
                            
                            termsUsed[categoryID][currentTerm]['synonyms'] = '"' + newSyn + '"'

                elif not ( re.search(r'^synonym:\s+"[^\"]+"\s+EXACT', line) is None ):
                    
                    newSyn = re.search(r'^synonym:\s+"([^\"]+)"\s+EXACT', line).group(1)

                    newSyn = re.sub(r'\\n', r' ', newSyn)

                    newSyn = newSyn.strip().strip('"\'').strip()

                    newSyn = re.sub(r'\"', r'', newSyn)

                    if re.search(r'\|', newSyn) is None:
                        
                        if 'synonyms' in termsUsed[categoryID][currentTerm]:
                            
                            termsUsed[categoryID][currentTerm]['synonyms'] = termsUsed[categoryID][currentTerm]['synonyms'] + '|"' + newSyn + '"'

                        else:
                            
                            termsUsed[categoryID][currentTerm]['synonyms'] = '"' + newSyn + '"'

                elif not ( re.search(r'^synonym:\s+"[^\"]+"\s+BROAD', line) is None ):

                    newSyn = re.search(r'^synonym:\s+"([^\"]+)"\s+BROAD', line).group(1)

                    newSyn = re.sub(r'\\n', r' ', newSyn)

                    newSyn = newSyn.strip().strip('"\'').strip()

                    newSyn = re.sub(r'\'', r'', newSyn)

                    if re.search(r'\|', newSyn) is None:

                        if 'synonyms' in termsUsed[categoryID][currentTerm]:
                            
                            termsUsed[categoryID][currentTerm]['synonyms'] = termsUsed[categoryID][currentTerm]['synonyms'] + '|"' + newSyn + '"'

                        else:
                            
                            termsUsed[categoryID][currentTerm]['synonyms'] = '"' + newSyn + '"'

                elif not ( re.search(r'^def:\s+', line) is None ):
                    
                    die('Unparsed def-line in %s OBO file: "%s"; aborting.' % ( cv, line ) )

            # end if ( line-type selector switch )

        # end for ( input file line iterator )

    # end with ( open refFile as IN for phenotype )

    for categoryID in categories:
        
        if categoryID == 'substance':
            
            progressReport("   PubChem and GlyTouCan substances and compounds [if you included substances and/or compounds in your submission, this step will take around 10 minutes, even with a fast disk]...")

            if len(termsUsed[categoryID]) > 0:
                
                cv = 'PubChem_substance'

                refFile = cvFile[cv]

                progressReport("      ...processing substance file... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

                with gzip.open( refFile, mode='rt' ) as IN:
                    
                    header = IN.readline()

                    for line in IN:
                        
                        line = line.rstrip('\r\n')

                        ( currentSID, currentName, currentDesc, currentSyn, currentLinkedCID ) = re.split(r'\t', line)

                        if currentSID in termsUsed[categoryID]:
                            
                            termsUsed[categoryID][currentSID]['name'] = currentName
                            termsUsed[categoryID][currentSID]['description'] = currentDesc
                            termsUsed[categoryID][currentSID]['synonyms'] = currentSyn
                            termsUsed[categoryID][currentSID]['compound'] = currentLinkedCID

                            # Initialize linked CID record for second-pass scan.

                            termsUsed['compound'][currentLinkedCID] = {}

                        # end if ( we loaded this SID during our input scan )

                    # end for ( line iterator on substance TSV )

                # end with ( open substance TSV for reading )

            # end if ( we have at least one substance term loaded )

            if len(termsUsed['compound']) > 0:
                
                progressReport("      ...processing compound file... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

                # Load substance-linked CID records, as well as those directly submitted, e.g. in collection_compound.

                cv = 'PubChem_and_GlyTouCan_compound'

                refFile = cvFile[cv]

                with gzip.open( refFile, mode='rt', encoding="latin-1" ) as IN:
                    
                    header = IN.readline()

                    for line in IN:
                        
                        line = line.rstrip('\r\n')

                        ( currentCID, currentName, currentDesc, currentSyn ) = re.split(r'\t', line)

                        if currentCID in termsUsed['compound']:
                            
                            termsUsed['compound'][currentCID]['name'] = currentName
                            termsUsed['compound'][currentCID]['description'] = currentDesc
                            termsUsed['compound'][currentCID]['synonyms'] = currentSyn

                        # end if ( we loaded this CID )

                    # end for ( line iterator on compound TSV )

                # end with ( open compound TSV for reading )

            # end if ( we have at least one compound term loaded )

        elif categoryID == 'gene':
            
            progressReport("   Ensembl genes... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

            if len(termsUsed[categoryID]) > 0:
                
                cv = 'Ensembl'

                refFile = cvFile[cv]

                with open( refFile, 'r' ) as IN:
                    
                    header = IN.readline()

                    for line in IN:
                        
                        line = line.rstrip('\r\n')

                        ( currentTerm, currentName, currentDesc, currentSyn, currentTaxID ) = re.split(r'\t', line)

                        if currentTerm in termsUsed[categoryID]:
                            
                            # Ensembl allows null term names; we do not. Auto-copy ID into name field if none exists.

                            if currentName == '':
                                
                                currentName = currentTerm

                            termsUsed[categoryID][currentTerm]['name'] = currentName
                            termsUsed[categoryID][currentTerm]['description'] = currentDesc
                            termsUsed[categoryID][currentTerm]['synonyms'] = currentSyn
                            termsUsed[categoryID][currentTerm]['organism'] = currentTaxID

                            termsUsed['ncbi_taxonomy'][currentTaxID] = {}

                        # end if ( current Ensembl gene ID was used in this submission )

                    # end for ( each line in the (CFDE-preprocessed-by-species) Ensembl gene DB TSV )

                # end with open( [gene reference TSV], 'r' ) as IN

        elif categoryID == 'protein':
            
            progressReport("   UniProtKB proteins... [if you included protein terms in your submission, this step will take around 10 minutes, even with a fast disk] [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

            if len(termsUsed[categoryID]) > 0:
                
                cv = 'UniProtKB'

                refFile = cvFile[cv]

                with gzip.open( refFile, mode='rt', encoding="latin-1" ) as IN:
                    
                    header = IN.readline()

                    for line in IN:
                        
                        line = line.rstrip('\r\n')

                        ( currentTerm, currentName, currentDesc, currentSyn, currentTaxID ) = re.split(r'\t', line)

                        if currentTerm in termsUsed[categoryID]:
                            
                            # Auto-copy ID into name field if none exists.

                            if currentName == '':
                                
                                currentName = currentTerm

                            termsUsed[categoryID][currentTerm]['name'] = currentName
                            termsUsed[categoryID][currentTerm]['description'] = currentDesc
                            termsUsed[categoryID][currentTerm]['synonyms'] = currentSyn
                            termsUsed[categoryID][currentTerm]['organism'] = currentTaxID

                            termsUsed['ncbi_taxonomy'][currentTaxID] = {}

                        # end if ( current UniProtKB protein ID was used in this submission )

                    # end for ( each line in the (CFDE-preprocessed) UniProtKB protein DB TSV )

                # end with open( [protein reference TSV], 'r' ) as IN

        elif categoryID == 'anatomy' or categoryID == 'assay_type' or categoryID == 'analysis_type' or categoryID == 'disease' or categoryID == 'sample_prep_method':
            
            cv = ''

            if categoryID == 'anatomy':
                
                progressReport("   Uber-anatomy ontology... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

                cv = 'Uberon'

            elif categoryID == 'assay_type':
                
                progressReport("   Ontology for Biomedical Investigations (assay type)... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

                # Load the provisional term map first for this one; afterward we'll
                # load the main ontology with destructive overwrites for any conflicts
                # (to defer to published term versions).

                cv = 'OBI_provisional'

            elif categoryID == 'analysis_type':
                
                progressReport("   Ontology for Biomedical Investigations (analysis type)... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

                # Load the provisional term map first for this one; afterward we'll
                # load the main ontology with destructive overwrites for any conflicts
                # (to defer to published term versions).

                cv = 'OBI_provisional'

            elif categoryID == 'sample_prep_method':
                
                progressReport("   Ontology for Biomedical Investigations (sample prep method)... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

                # Load the provisional term map first for this one; afterward we'll
                # load the main ontology with destructive overwrites for any conflicts
                # (to defer to published term versions).

                cv = 'OBI_provisional'

            elif categoryID == 'disease':
                
                progressReport("   Disease Ontology... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

                cv = 'DO'

            # end if ( categoryID type check )

            if cv == 'OBI_provisional':
                
                refFile = cvFile[cv]

                with open( refFile, 'r' ) as IN:
                    
                    header = IN.readline()

                    for line in IN:
                        
                        line = line.rstrip('\r\n')

                        ( termRequester, termStatus, currentTerm, termLabel, termDefinition, termSynonyms ) = re.split(r'\t', line)

                        if currentTerm in termsUsed[categoryID]:
                            
                            termsUsed[categoryID][currentTerm]['name'] = termLabel
                            termsUsed[categoryID][currentTerm]['description'] = termDefinition

                            if termSynonyms == '':
                                
                                termSynonyms = '[]'

                            termsUsed[categoryID][currentTerm]['synonyms'] = termSynonyms

                        # end if ( term was seen )

                    # end for ( each line in provisional OBI term TSV )

                # end with ( open [provisional OBI term TSV] as IN

                cv = 'OBI'

            # end if cv == 'OBI_provisional'

            refFile = cvFile[cv]

            with open( refFile, 'r' ) as IN:
                
                recording = False

                currentTerm = ''

                for line in IN:
                    
                    line = line.rstrip('\r\n')
                
                    matchResult = re.search(r'^id:\s+(\S.*)$', line)

                    if not( matchResult is None ):
                        
                        currentTerm = matchResult.group(1)

                        if currentTerm in termsUsed[categoryID]:
                            
                            recording = True

                            # Wipe out any synonyms we already loaded from a provisional term list, if we did that.
                            # (During EDAM processing (below), a destructive overwrite already happens by default
                            # for this case, so no extra deletion is required.)

                            if 'synonyms' in termsUsed[categoryID][currentTerm]:
                                
                                del termsUsed[categoryID][currentTerm]['synonyms']

                        else:
                            
                            currentTerm = ''

                            # (Recording is already switched off by default.)

                    elif not( re.search(r'^\[Term\]', line) is None ):
                        
                        recording = False

                    elif not( re.search(r'^\[Typedef\]', line) is None ):
                        
                        break

                    elif recording:
                        
                        if not ( re.search(r'^name:\s+(\S*.*)$', line) is None ):
                            
                            termsUsed[categoryID][currentTerm]['name'] = re.search(r'^name:\s+(\S*.*)$', line).group(1)

                        elif not ( re.search(r'^def:\s+\"(.*)\"[^\"]*$', line) is None ):
                            
                            parsedDesc = re.search(r'^def:\s+\"(.*?)(?<!\\)\".*$', line).group(1)

                            if currentTerm == 'UBERON:4300002':
                                
                                # Until this is fixed in the ontology, a typo involving unpaired brackets
                                # makes this one impossible to parse properly.

                                parsedDesc = re.sub(r'\]$', r'', parsedDesc)

                            parsedDesc = re.sub(r'\s*\[[^\]]+\]', r'', parsedDesc)

                            # Remove newline codes and replace with space characters.

                            parsedDesc = re.sub(r'\\n', r' ', parsedDesc)

                            # Trim remaining extremal whitespace.

                            parsedDesc = re.sub(r'^\s+', r'', parsedDesc)
                            parsedDesc = re.sub(r'\s+$', r'', parsedDesc)

                            termsUsed[categoryID][currentTerm]['description'] = parsedDesc

                        elif not ( re.search(r'^alt_id:\s+\S+', line) is None ):
                            
                            newSyn = re.search(r'^alt_id:\s+(\S+)', line).group(1)

                            newSyn = re.sub(r'\"', r'', newSyn)
                            newSyn = re.sub(r'\'', r'', newSyn)

                            if re.search(r'\|', newSyn) is None:
                                
                                if 'synonyms' in termsUsed[categoryID][currentTerm]:
                                    
                                    termsUsed[categoryID][currentTerm]['synonyms'] = termsUsed[categoryID][currentTerm]['synonyms'] + '|"' + newSyn + '"'

                                else:
                                    
                                    termsUsed[categoryID][currentTerm]['synonyms'] = '"' + newSyn + '"'

                        elif not ( re.search(r'^synonym:\s+"[^\"]+"\s+EXACT', line) is None ):
                            
                            newSyn = re.search(r'^synonym:\s+"([^\"]+)"\s+EXACT', line).group(1)

                            newSyn = re.sub(r'\\n', r' ', newSyn)

                            newSyn = newSyn.strip().strip('"\'').strip()

                            newSyn = re.sub(r'\"', r'', newSyn)

                            if re.search(r'\|', newSyn) is None:
                                
                                if 'synonyms' in termsUsed[categoryID][currentTerm]:
                                    
                                    termsUsed[categoryID][currentTerm]['synonyms'] = termsUsed[categoryID][currentTerm]['synonyms'] + '|"' + newSyn + '"'

                                else:
                                    
                                    termsUsed[categoryID][currentTerm]['synonyms'] = '"' + newSyn + '"'

                        elif not ( re.search(r'^synonym:\s+"[^\"]+"\s+BROAD', line) is None ):

                            newSyn = re.search(r'^synonym:\s+"([^\"]+)"\s+BROAD', line).group(1)

                            newSyn = re.sub(r'\\n', r' ', newSyn)

                            newSyn = newSyn.strip().strip('"\'').strip()

                            newSyn = re.sub(r'\'', r'', newSyn)

                            if re.search(r'\|', newSyn) is None:

                                if 'synonyms' in termsUsed[categoryID][currentTerm]:
                                    
                                    termsUsed[categoryID][currentTerm]['synonyms'] = termsUsed[categoryID][currentTerm]['synonyms'] + '|"' + newSyn + '"'

                                else:
                                    
                                    termsUsed[categoryID][currentTerm]['synonyms'] = '"' + newSyn + '"'

                        elif not ( re.search(r'^def:\s+', line) is None ):
                            
                            die('Unparsed def-line in %s OBO file: "%s"; aborting.' % ( cv, line ) )

                    # end if ( line-type selector switch )

                # end for ( input file line iterator )

            # end with ( open refFile as IN )

        elif categoryID == 'file_format' or categoryID == 'data_type':
            
            if categoryID == 'file_format':
                
                progressReport("   EDAM/Interlex (file format)... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

            else:
                
                progressReport("   EDAM/Interlex (data type)... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

            cv = 'EDAM'

            refFile = cvFile[cv]

            with open( refFile, 'r' ) as IN:
                
                header = IN.readline()

                for line in IN:
                    
                    line = line.rstrip('\r\n')

                    ( termURL, name, synonymBlock, definition ) = re.split(r'\t', line)[0:4]

                    currentTerm = re.sub(r'^.*\/([^\/]+)$', r'\1', termURL)

                    currentTerm = re.sub(r'data_', r'data:', currentTerm)
                    currentTerm = re.sub(r'format_', r'format:', currentTerm)

                    if currentTerm in termsUsed[categoryID]:
                        
                        # There are some truly screwy things allowed inside
                        # tab-separated fields in this file. Clean them up.

                        name = name.strip().strip('"\'').strip()

                        synonymBlock = synonymBlock.strip().strip('"\'').strip()

                        synonymField = ''

                        if synonymBlock == '':
                            
                            synonymField = '[]'

                        else:
                            
                            synonyms = re.split(r'\|+', synonymBlock)

                            synonymField = '['

                            first = 1

                            for synonym in synonyms:
                                
                                synonym = re.sub(r'"', r'', synonym)

                                if first == 1:
                                    
                                    synonymField = synonymField + '"' + synonym + '"'

                                    first = 0

                                else:
                                    
                                    synonymField = synonymField + ',"' + synonym + '"'

                            synonymField = synonymField + ']'

                        definition = definition.strip().strip('"\'').strip()

                        definition = re.sub( r'\|.*$', r'', definition )

                        if categoryID == 'file_format' and not( re.search(r'^format', currentTerm) is None ):
                            
                            termsUsed[categoryID][currentTerm] = {}

                            termsUsed[categoryID][currentTerm]['name'] = name
                            termsUsed[categoryID][currentTerm]['description'] = definition
                            termsUsed[categoryID][currentTerm]['synonyms'] = synonymField

                        elif categoryID == 'data_type' and not( re.search(r'^data', currentTerm) is None ):
                            
                            termsUsed[categoryID][currentTerm] = {}

                            termsUsed[categoryID][currentTerm]['name'] = name
                            termsUsed[categoryID][currentTerm]['description'] = definition
                            termsUsed[categoryID][currentTerm]['synonyms'] = synonymField

                    # end if ( currentTerm in termsUsed[categoryID] )

                # end for ( input file line iterator )

            # end with ( refFile opened as IN )

            if categoryID == 'file_format' or categoryID == 'data_type':
                
                # Load auxiliary terms from the Interlex TSV.

                cv = 'Interlex_%s' % categoryID

                refFile = cvFile[cv]

                with open( refFile, 'r' ) as IN:
                    
                    header = IN.readline()

                    for line in IN:
                        
                        line = line.rstrip('\r\n')

                        # Interlex_ids	term_name	Slim_term_id	Slim_term_label	definition	parent_ids	parent_names	suggested_by group	contact person(s)

                        ( currentTerm, name, slimID, slimLabel, description ) = re.split(r'\t', line)[0:5]

                        if currentTerm in termsUsed[categoryID]:
                            
                            synonymField = '[]'
                            definition = definition.strip().strip('"\'').strip()
                            
                            termsUsed[categoryID][currentTerm] = {}

                            termsUsed[categoryID][currentTerm]['name'] = name
                            termsUsed[categoryID][currentTerm]['description'] = description
                            termsUsed[categoryID][currentTerm]['synonyms'] = synonymField

                        # end if ( currentTerm in termsUsed[categoryID] )

                    # end for ( input file line iterator )

                # end with ( refFile opened as IN )

            # end if ( categoryID == 'file_format' or categoryID == 'data_type' )

        # end if ( switch on categoryID )

    # end foreach ( categoryID in termsUsed )

    # NCBI GOES HERE -- must follow finished processing of protein, gene

    categoryID = 'ncbi_taxonomy'
        
    progressReport("   NCBI Taxonomy... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

    cv = 'NCBI'

    refFile = cvFile[cv]

    with gzip.open( refFile, mode='rt' ) as IN:
        
        header = IN.readline()

        for line in IN:
            
            line = line.rstrip('\r\n')

            ( currentTerm, currentClade, currentName, currentDesc, currentSyn ) = re.split(r'\t', line)

            if currentTerm in termsUsed[categoryID]:
                
                termsUsed[categoryID][currentTerm]['clade'] = currentClade
                termsUsed[categoryID][currentTerm]['name'] = currentName
                termsUsed[categoryID][currentTerm]['description'] = currentDesc
                termsUsed[categoryID][currentTerm]['synonyms'] = currentSyn

            # end if ( current NCBI taxon ID was used in this submission )

        # end for ( each line in the NCBI Taxonomy DB TSV )

    # end with open( [taxonomy reference TSV], 'r' ) as IN

    progressReport("\n...done loading display data for all CVs. [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

# end sub decorateTermsUsed(  )

# Write auto-constructed tables linking CV terms across ontologies.

def writeRefMaps(  ):
    
    global outDir, refMap

    for source in refMap:
        
        for dest in refMap[source]:
            
            outFile = '%s/%s_%s.tsv' % ( outDir, source, dest )

            with open( outFile, 'w' ) as OUT:
                
                OUT.write( '\t'.join( [source, dest] ) + '\n' )

                for source_id in refMap[source][dest]:
                    
                    for dest_id in refMap[source][dest][source_id]:
                        
                        OUT.write( '\t'.join( [source_id, dest_id] ) + '\n' )

# end sub writeRefMaps(  )

def writeTermsUsed(  ):
    
    global outDir, termsUsed, errorStrings

    progressReport("Writing CV term tracker tables (to be included in complete C2M2 submission)... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]\n")

    for categoryID in termsUsed:
        
        outFile = '%s/%s.tsv' % ( outDir, categoryID )

        progressReport("   writing \"" + outFile + "\"... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

        with open(outFile, 'w') as OUT:
            
            if categoryID == 'substance':
                
                OUT.write( '\t'.join( [ 'id', 'name', 'description', 'synonyms', 'compound' ] ) + '\n' )

            elif categoryID == 'ncbi_taxonomy':
                
                OUT.write( '\t'.join( [ 'id', 'clade', 'name', 'description', 'synonyms' ] ) + '\n' )

            elif categoryID == 'gene':
                
                OUT.write( '\t'.join( [ 'id', 'name', 'description', 'synonyms', 'organism' ] ) + '\n' )

            elif categoryID == 'protein':
                
                OUT.write( '\t'.join( [ 'id', 'name', 'description', 'synonyms', 'organism' ] ) + '\n' )

            else:
                
                OUT.write( '\t'.join( [ 'id', 'name', 'description', 'synonyms' ] ) + '\n' )

            for termID in sorted( termsUsed[categoryID] ):
                
                termDesc = ''

                termSynonyms = '[]'

                # If 'name' is blank, this is a universe-level error!

                if 'name' in termsUsed[categoryID][termID] and termsUsed[categoryID][termID]['name'] != '':
                    
                    if 'description' in termsUsed[categoryID][termID]:
                        
                        termDesc = termsUsed[categoryID][termID]['description']

                    if 'synonyms' in termsUsed[categoryID][termID]:
                        
                        termSynonyms = termsUsed[categoryID][termID]['synonyms']

                        # Synonyms for the categories excluded here are preprocessed by us in advance and shouldn't need help.

                        if categoryID != 'ncbi_taxonomy' and categoryID != 'gene' and categoryID != 'protein' and categoryID != 'substance' and categoryID != 'compound':
                            
                            if termSynonyms != '' and re.search(r'\|', termSynonyms) is None and re.search(r'^\[', termSynonyms) is None:
                                
                                termSynonyms = '[' + termSynonyms + ']'

                            elif termSynonyms != '' and not(re.search(r'\|', termSynonyms) is None):
                                
                                synonyms = re.split(r'\|+', termSynonyms)

                                termSynonyms = '['

                                first = 1

                                for synonym in synonyms:
                                    
                                    if first == 1:
                                        
                                        termSynonyms = termSynonyms + synonym

                                        first = 0

                                    else:
                                        
                                        termSynonyms = termSynonyms + ',' + synonym

                                termSynonyms = termSynonyms + ']'

                    # end field preprocessing

                    # Uncomment the following line if you get a weird error: it'll report all the CV terms processed so far
                    # (as you're running the script), and should stop after the first one that causes an error. (If there are any.)

                    # progressReport(termID)

                    if categoryID == 'ncbi_taxonomy':
                        
                        OUT.write( '\t'.join( [ termID, termsUsed[categoryID][termID]['clade'], termsUsed[categoryID][termID]['name'], termDesc, termSynonyms ] ) + '\n' )

                    elif categoryID == 'gene':
                        
                        OUT.write( '\t'.join( [ termID, termsUsed[categoryID][termID]['name'], termDesc, termSynonyms, termsUsed[categoryID][termID]['organism'] ] ) + '\n' )

                    elif categoryID == 'protein':
                        
                        OUT.write( '\t'.join( [ termID, termsUsed[categoryID][termID]['name'], termDesc, termSynonyms, termsUsed[categoryID][termID]['organism'] ] ) + '\n' )

                    elif categoryID == 'substance':
                        
                        OUT.write( '\t'.join( [ termID, termsUsed[categoryID][termID]['name'], termDesc, termSynonyms, termsUsed[categoryID][termID]['compound'] ] ) + '\n' )

                    else:
                        
                        OUT.write( '\t'.join( [ termID, termsUsed[categoryID][termID]['name'], termDesc, termSynonyms ] ) + '\n' )

                else:
                    
                    cacheError('Term \'%s\' has no name: if the term ID isn\'t a typo, please check integrity of \'%s\' reference file(s).' % ( termID, categoryID ) )

                # end if ( 'name' in termsUsed[categoryID][termID] )

            # end for ( termID in termsUsed[categoryID] )

        # end with ( open(outFile, 'w') as OUT )

    # end for ( categoryID in termsUsed )

    if len(errorStrings) > 0:
        
        deferredDie()

    progressReport("\n...done writing term tables for final C2M2 submission datapackage. [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

# end sub writeTermsUsed(  )

# Make sure every file record with a persistent ID has a non-null checksum.

def checkChecksums(  ):
    
    global submissionDraftDir, errorStrings

    inFile = submissionDraftDir + '/file.tsv'

    progressReport("Ensuring all file.tsv records with persistent IDs have non-null checksums... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

    with open( inFile, 'r' ) as IN:
        
        header = IN.readline()

        lineCount = 0

        for line in IN:
            
            lineCount = lineCount + 1

            line = line.rstrip('\r\n')

            ( id_namespace, local_id, project_id_namespace, project_local_id, persistent_id, creation_time, size_in_bytes, uncompressed_size_in_bytes, sha256, md5, filename, file_format, compression_format, data_type, assay_type, analysis_type, mime_type, bundle_collection_id_namespace, bundle_collection_local_id, dbgap_study_id ) = re.split(r'\t', line)

            if persistent_id != '' and sha256 == '' and md5 == '':
                
                cacheError("file.tsv line %s: file record \"%s%s\" has a non-null persistent ID (\"%s\"), but both sha256 and md5 are blank." % ( lineCount, id_namespace, local_id, persistent_id ) )

    if len(errorStrings) > 0:
        
        deferredDie()

    progressReport("\n...done; all file records passed checksum verification. [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

# end sub checkChecksums(  )

# Ensure absolute uniqueness of persistent IDs in this submission (across tables).

def checkUniquePersistentIDs(  ):

    global submissionDraftDir

    progressReport("Ensuring all persistent IDs are unique (both within and across tables)... [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

    seen = {}

    for basename in ( 'file.tsv', 'biosample.tsv', 'subject.tsv', 'collection.tsv', 'project.tsv' ):
        
        inFile = '%s/%s' % ( submissionDraftDir, basename )

        with open( inFile, 'r' ) as IN:
            
            header = IN.readline()

            colnames = re.split(r'\t', header.rstrip('\r\n'))

            targetIndex = -1

            currentIndex = 0

            for colname in colnames:
                
                if colname == 'persistent_id':
                    
                    targetIndex = currentIndex

                currentIndex = currentIndex + 1

            if targetIndex == -1:
                
                die("If you're seeing this, you're missing a required persistent_id column in one or more of file.tsv, biosample.tsv, subject.tsv, collection.tsv or project.tsv.")

            lineCount = 0

            for line in IN:
                
                lineCount = lineCount + 1

                line = line.rstrip('\r\n')

                fields = re.split(r'\t', line)

                currentPersistentID = fields[targetIndex]

                if currentPersistentID != '':
                    
                    localID = fields[0] + fields[1]

                    if currentPersistentID in seen:
                        
                        lastID = seen[currentPersistentID]

                        cacheError("Persistent ID \"%s\" is attached to two distinct records (\"%s\" and \"%s\"; the latter is in \"%s\", line %s." % ( currentPersistentID, lastID, localID, inFile, lineCount ))

                    else:
                        
                        seen[currentPersistentID] = localID

    if len(errorStrings) > 0:
        
        deferredDie()

    progressReport("\n...done; all persistent IDs verified to be unique. [" + subprocess.check_output(['date']).decode(sys.stdout.encoding).rstrip('\r\n') + "]")

# end sub checkUniquePersistentIDs(  )

##########################################################################################
##########################################################################################
##########################################################################################
#                                                    EXECUTION
##########################################################################################
##########################################################################################
##########################################################################################

# Create the output directory if need be.

if not os.path.isdir(outDir) and os.path.exists(outDir):
    
    die('%s exists but is not a directory; aborting.' % outDir)

elif not os.path.isdir(outDir):
    
    os.mkdir(outDir)

# Find all the CV terms used in the draft C2M2 submission in "submissionDraftDir".

identifyTermsUsed()

# Load data from CV reference files to fill out needed columns in C2M2
# term-tracker tables.

decorateTermsUsed()

# Write the tables of inter-CV reference links.

writeRefMaps()

# Write the term-tracker tables.

writeTermsUsed()

##########################################################################################
# EXTRA-SCHEMATIC VALIDATIONS

# Check to make sure all file records with persistent IDs have non-null checksums.

checkChecksums()

# Check to make sure all persistent IDs are unique (across as well as within tables)

checkUniquePersistentIDs()


